{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4b2e41-d0b4-4d37-bc7d-85c2a6dbeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from pptx import Presentation\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.util import Cm, Pt, Inches\n",
    "from pptx.enum.text import MSO_ANCHOR, MSO_AUTO_SIZE\n",
    "from h2ogpte import H2OGPTE\n",
    "from mediawikiapi import MediaWikiAPI\n",
    "from tqdm import tqdm\n",
    "with open('secrets.txt') as f:\n",
    "    api = f.read()\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2c9a1b-6481-447e-9513-732dc1e44eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = H2OGPTE(\n",
    "    address=\"https://h2ogpte.genai.h2o.ai\",\n",
    "    api_key=api\n",
    ")\n",
    "\n",
    "user_query = 'Create a presentation on the transformers franchise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c753be01-40dc-4a03-8d7a-5af7397ccdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_randomly(lst, retain=30):\n",
    "    '''\n",
    "    Shuffles a list and randomly\n",
    "    '''\n",
    "    _ = lst.copy()\n",
    "    random.shuffle(_)\n",
    "    return _[0:retain]\n",
    "\n",
    "\n",
    "\n",
    "def try_and_parse(user_query, function, failed=0, markdown=False):\n",
    "    '''\n",
    "    Accepts a function and user_query, an input. Evaluates function(user_query) and \n",
    "    converts string output (usually a reply from an llm) into a json value\n",
    "    '''\n",
    "    chosen = function(user_query)\n",
    "    try:\n",
    "        if not markdown:\n",
    "            topics = json.loads(chosen.content)\n",
    "        else:\n",
    "            print(chosen.content)\n",
    "            pattern = r'^```(?:\\w+)?\\s*\\n(.*?)(?=^```)```'\n",
    "            result = re.findall(pattern, chosen.content, re.DOTALL | re.MULTILINE)[0].strip() \n",
    "            topics = json.loads(result)\n",
    "            \n",
    "        return topics\n",
    "    except Exception as e:\n",
    "        failed+=1\n",
    "        print(failed)\n",
    "        print(e)# CHANGE TO LOGGING STATEMENT\n",
    "        try_and_parse(user_query, function, failed=failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0070654e-5a34-45a9-a233-d924424311c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SessionError",
     "evalue": "RuntimeError: ['Traceback (most recent call last):\\n', '  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 798, in _inner\\n    predictions = _predict(*data)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 829, in _predict\\n    raise ValueError(result[\"error\"])\\n', \"ValueError: Invalid model mistral-large, valid models are: ['mistralai/Mixtral-8x7B-Instruct-v0.1', 'h2oai/h2ogpt-4096-llama2-70b-chat', 'h2oai/h2ogpt-4096-llama2-13b-chat', 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'HuggingFaceH4/zephyr-7b-beta', 'NousResearch/Nous-Capybara-34B', 'claude-2.1', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k-0613', 'gpt-35-turbo-1106', 'gpt-4-1106-preview', 'gemini-pro']\\n\"]\nTraceback (most recent call last):\n  File \"src/h2ogpte_core/server.py\", line 205, in h2ogpte_core.server.answer_question_using_context\n  File \"src/h2ogpte_core/h2ogpt.py\", line 56, in query\n  File \"src/h2ogpte_core/h2ogpt.py\", line 579, in query_or_summarize_or_extract\n  File \"src/h2ogpte_core/h2ogpt.py\", line 547, in h2ogpte_core.h2ogpt.query_or_summarize_or_extract\n  File \"src/h2ogpte_core/grclient.py\", line 57, in h2ogpte_core.grclient.check_job\nRuntimeError: ['Traceback (most recent call last):\\n', '  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 798, in _inner\\n    predictions = _predict(*data)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 829, in _predict\\n    raise ValueError(result[\"error\"])\\n', \"ValueError: Invalid model mistral-large, valid models are: ['mistralai/Mixtral-8x7B-Instruct-v0.1', 'h2oai/h2ogpt-4096-llama2-70b-chat', 'h2oai/h2ogpt-4096-llama2-13b-chat', 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'HuggingFaceH4/zephyr-7b-beta', 'NousResearch/Nous-Capybara-34B', 'claude-2.1', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k-0613', 'gpt-35-turbo-1106', 'gpt-4-1106-preview', 'gemini-pro']\\n\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSessionError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m user_query: client\u001b[38;5;241m.\u001b[39manswer_question(\n\u001b[0;32m      2\u001b[0m     question\u001b[38;5;241m=\u001b[39muser_query,\n\u001b[0;32m      3\u001b[0m     system_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant whose task is to perform Wikipedia searches on a specific topic.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral-large\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# i like this model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m searched \u001b[38;5;241m=\u001b[39m try_and_parse(user_query, search)\n\u001b[0;32m     18\u001b[0m searched\n",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mtry_and_parse\u001b[1;34m(user_query, function, failed, markdown)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_and_parse\u001b[39m(user_query, function, failed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, markdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    Accepts a function and user_query, an input. Evaluates function(user_query) and \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    converts string output (usually a reply from an llm) into a json value\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m function(user_query)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m markdown:\n",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(user_query)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m user_query: client\u001b[38;5;241m.\u001b[39manswer_question(\n\u001b[0;32m      2\u001b[0m     question\u001b[38;5;241m=\u001b[39muser_query,\n\u001b[0;32m      3\u001b[0m     system_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant whose task is to perform Wikipedia searches on a specific topic.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m    The user is interested to create a presentation about a topic of interest.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m    Reply with at least one corresponding Wikipedia query as an array in JSON format.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    Only reply with the JSON array and nothing else. Here are some examples.\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m    Example 1.\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    User: Create a presentation on the book, Baby Rudin.\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    Assistant: [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMathematical Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m    Example 2.\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m    User: I want to create a ppt about Milk.\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    Assistant: [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMilk\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlant Milk\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnimal Milk\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlmond Milk\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral-large\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# i like this model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m searched \u001b[38;5;241m=\u001b[39m try_and_parse(user_query, search)\n\u001b[0;32m     18\u001b[0m searched\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsa4213\\Lib\\site-packages\\h2ogpte\\h2ogpte.py:337\u001b[0m, in \u001b[0;36mH2OGPTE.answer_question\u001b[1;34m(self, question, system_prompt, pre_prompt_query, prompt_query, text_context_list, llm, llm_args, chat_conversation, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    335\u001b[0m     ], key\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SessionError(ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Answer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mret)\n",
      "\u001b[1;31mSessionError\u001b[0m: RuntimeError: ['Traceback (most recent call last):\\n', '  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 798, in _inner\\n    predictions = _predict(*data)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 829, in _predict\\n    raise ValueError(result[\"error\"])\\n', \"ValueError: Invalid model mistral-large, valid models are: ['mistralai/Mixtral-8x7B-Instruct-v0.1', 'h2oai/h2ogpt-4096-llama2-70b-chat', 'h2oai/h2ogpt-4096-llama2-13b-chat', 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'HuggingFaceH4/zephyr-7b-beta', 'NousResearch/Nous-Capybara-34B', 'claude-2.1', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k-0613', 'gpt-35-turbo-1106', 'gpt-4-1106-preview', 'gemini-pro']\\n\"]\nTraceback (most recent call last):\n  File \"src/h2ogpte_core/server.py\", line 205, in h2ogpte_core.server.answer_question_using_context\n  File \"src/h2ogpte_core/h2ogpt.py\", line 56, in query\n  File \"src/h2ogpte_core/h2ogpt.py\", line 579, in query_or_summarize_or_extract\n  File \"src/h2ogpte_core/h2ogpt.py\", line 547, in h2ogpte_core.h2ogpt.query_or_summarize_or_extract\n  File \"src/h2ogpte_core/grclient.py\", line 57, in h2ogpte_core.grclient.check_job\nRuntimeError: ['Traceback (most recent call last):\\n', '  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 798, in _inner\\n    predictions = _predict(*data)\\n', '  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\", line 829, in _predict\\n    raise ValueError(result[\"error\"])\\n', \"ValueError: Invalid model mistral-large, valid models are: ['mistralai/Mixtral-8x7B-Instruct-v0.1', 'h2oai/h2ogpt-4096-llama2-70b-chat', 'h2oai/h2ogpt-4096-llama2-13b-chat', 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'HuggingFaceH4/zephyr-7b-beta', 'NousResearch/Nous-Capybara-34B', 'claude-2.1', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k-0613', 'gpt-35-turbo-1106', 'gpt-4-1106-preview', 'gemini-pro']\\n\"]\n"
     ]
    }
   ],
   "source": [
    "search = lambda user_query: client.answer_question(\n",
    "    question=user_query,\n",
    "    system_prompt=\"\"\"You are an assistant whose task is to perform Wikipedia searches on a specific topic.\\\n",
    "    The user is interested to create a presentation about a topic of interest.\\\n",
    "    Reply with at least one corresponding Wikipedia query as an array in JSON format.\\\n",
    "    Only reply with the JSON array and nothing else. Here are some examples.\n",
    "    Example 1.\n",
    "    User: Create a presentation on the book, Baby Rudin.\n",
    "    Assistant: [\"Real analysis\", \"Mathematical Analysis\"]\n",
    "\n",
    "    Example 2.\n",
    "    User: I want to create a ppt about Milk.\n",
    "    Assistant: [\"Milk\", \"Plant Milk\", \"Animal Milk\", \"Almond Milk\"]\n",
    "    \"\"\",\n",
    "    llm='mistralai/Mixtral-8x7B-Instruct-v0.1' # i like this model\n",
    ")\n",
    "searched = try_and_parse(user_query, search)\n",
    "searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774e298-43a4-410d-8ca5-cab51af71220",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = MediaWikiAPI()\n",
    "\n",
    "articles = list(\n",
    "    set(\n",
    "        \n",
    "        [i for j in [wiki.search(cat, results=5) for cat in searched] for i in j]\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# remove duplicates with set(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc504cd6-6449-47e4-87cf-45d4385c39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "snippet = trim_randomly(\n",
    "    list(map(lambda x: wiki.summary(x, auto_suggest=False, sentences=1), articles))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "snippet_ = snippet.copy()\n",
    "i = 0\n",
    "for string in snippet:\n",
    "    string = f\"{i}. {string}\"\n",
    "    snippet[i] = string\n",
    "    i+=1\n",
    "snippet_text = \"\\n\\n\".join(snippet)\n",
    "print(snippet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a940365-1bd4-4a4b-b6af-0aa869ab6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_topics = lambda user_query: client.answer_question(\n",
    "        question=f\"\"\"{user_query}\n",
    "        Referring to the list of wikipedia entries you have been provided, decide on which topics are useful for the presentation. For each entry, explain, in a few words,\\\n",
    "        whether you think an entry is useful or not and why.\n",
    "        After that, generate a code chunk. Within the code chunk is an array of integers in JSON, and these integers correspond to the topics you think are useful. \n",
    "        Please keep strictly to the format in the following example:\n",
    "        0. - Not useful, sugar irrelevant to Jesus Christ\n",
    "        1. - Useful, christianity is about the topic of Jesus Christ\n",
    "        2. - Useful, protestants follow Jesus Christ\n",
    "        ```\n",
    "        [1, 2]\n",
    "        ```\n",
    "        \"\"\",\n",
    "        system_prompt=f\"\"\"You are an assistant whose task is to help a user in creating a presentation.\\\n",
    "        Here are a list of wikipedia entries, starting from the 0th entry, that may or may not be related to the topic at hand:\n",
    "        {snippet_text}\n",
    "        \"\"\",\n",
    "        llm='gpt-4-1106-preview' \n",
    "    )\n",
    "\n",
    "topics = try_and_parse(user_query, choose_topics, markdown=True)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e216c3-a3fa-4755-91ed-5b56256d67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e8a6a-26f2-4915-821e-9ca9e01d7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_articles = [articles[i] for i in topics]\n",
    "chosen_snippets = [snippet_[i] for i in topics]\n",
    "chosen_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b730249-875b-4ed0-93fc-16c2d4fd163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_full_articles = list(map(lambda x: wiki.page(x, auto_suggest=False).content, chosen_articles))\n",
    "chosen_articles_images = list(map(lambda x: wiki.page(x, auto_suggest=False).images, chosen_articles))\n",
    "# # Create documents\n",
    "# # Note: Done for demonstration purposes only (not usually needed)\n",
    "# with open('dunder_mifflin.txt', 'w') as f:\n",
    "#     f.write('There were 55 paper clips shipped, 22 to Scranton and 33 to Filmer.')\n",
    "    \n",
    "# with open('initech.txt', 'w') as f:\n",
    "#     f.write('David Brent did not sign any contract with Initech.')\n",
    "    \n",
    "# # Upload documents\n",
    "# # Many file types are supported: text/image/audio documents and archives\n",
    "# with open('dunder_mifflin.txt', 'rb') as f:\n",
    "#     dunder_mifflin = client.upload('Dunder Mifflin.txt', f)\n",
    "    \n",
    "# with open('initech.txt', 'rb') as f:\n",
    "#     initech = client.upload('IniTech.txt', f)\n",
    "\n",
    "# # Ingest documents (Creates previews, chunks and embeddings)\n",
    "# client.ingest_uploads(collection_id, [dunder_mifflin, initech])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15055ab0-f43b-4f6a-849c-c6a98ad6eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# now its time to store them for RAG\n",
    "import os\n",
    "\n",
    "\n",
    "collection_id = client.create_collection(\n",
    "    name='Articles',\n",
    "    description='wikipedia articles for presentation',\n",
    ")\n",
    "\n",
    "pages = dict(zip(chosen_articles, chosen_full_articles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdbe96-7895-41be-9566-d99c4d03f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "to_ingest = []\n",
    "for title, content in pages.items():\n",
    "    title = re.sub('[\\W_]+', '', title)\n",
    "    name = f\"./articles/{title}.txt\"\n",
    "    f = open(name, \"w+\", encoding=\"utf-8\")\n",
    "    f.write(content)\n",
    "    f.close() # dont know why i gotta do this, i think it has to be in binary\n",
    "    f = open(name, 'rb')\n",
    "    to_ingest.append(client.upload(name, f))\n",
    "    print(f\"{name} fed!\")\n",
    "    f.close() \n",
    "\n",
    "client.ingest_uploads(collection_id, to_ingest)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc21a2-68de-49eb-8ff2-93923e71072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a chat session\n",
    "# chat_session_id = client.create_chat_session(collection_id)\n",
    "\n",
    "# # Query the collection\n",
    "# with client.connect(chat_session_id) as session:\n",
    "#     reply = session.query(\n",
    "#         'How many paper clips were shipped to Scranton?',\n",
    "#         llm=\"gpt-4-0613\"\n",
    "#     )\n",
    "#     print(reply.content)\n",
    "\n",
    "#     reply = session.query(\n",
    "#         'Did David Brent co-sign the contract with Initech?',\n",
    "#         timeout=60,\n",
    "#         llm=\"gpt-4-0613\"\n",
    "#     )\n",
    "#     print(reply.content)\n",
    "\n",
    "# # Summarize each document\n",
    "# documents = client.list_documents_in_collection(collection_id, offset=0, limit=99)\n",
    "# for doc in documents:\n",
    "#     summary = client.summarize_document(\n",
    "#         document_id=doc.id,\n",
    "#         timeout=60,\n",
    "#     )\n",
    "#     print(summary.content)\n",
    "\n",
    "\n",
    "#client.delete_documents_from_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68204587-8e88-4729-a520-98c429f02b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824ca52-ae9f-4104-859b-cd7da908d148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a7ba5-2cdf-4f4e-b811-2d17abf3f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_sections = lambda user_query: client.answer_question(\n",
    "        question=f\"\"\"{user_query}\n",
    "        Please plan the presentation by replying a JSON array \\\n",
    "        consisting of slide titles, starting from the first slide to the last slide. Do not include the title slide. Do not say anything else.\n",
    "\n",
    "        Here is an example reply:\n",
    "        [\"Introduction to Cookies\", \"History of cookies\", \"Types of Cookies\", \"Health Concerns\", \"Conclusion\"]  \n",
    "        \"\"\",\n",
    "        system_prompt=f\"\"\"You are an assistant whose task is to help a user in creating a presentation.\\\n",
    "        Here are a list of wikipedia entry summaries that are selected for the presentation:\n",
    "        {chosen_snippets}\n",
    "        You will be asked to come up with slide titles for the presentation.\n",
    "        \"\"\",\n",
    "        llm='mistralai/Mixtral-8x7B-Instruct-v0.1' \n",
    "    )\n",
    "\n",
    "sections = try_and_parse(user_query, decide_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac776f-de82-461d-9313-e671d1ae2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e83322-475c-4678-8f4c-f822ab5e5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session_id = client.create_chat_session(collection_id)\n",
    "chat_session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cbc7b-fe01-4260-a454-b7711c8ffa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ref for slide types:  \n",
    "0 ->  title and subtitle \n",
    "1 ->  title and content \n",
    "2 ->  section header \n",
    "3 ->  two content \n",
    "4 ->  Comparison \n",
    "5 ->  Title only  \n",
    "6 ->  Blank \n",
    "7 ->  Content with caption \n",
    "8 ->  Pic with caption \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c297ad-f4ef-4aac-a719-e8f2c8237a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation()\n",
    "prs.slide_width = Inches(16)\n",
    "prs.slide_height = Inches(9)\n",
    "title_slide = prs.slides.add_slide(prs.slide_layouts[0]) \n",
    "decide_slide_format = lambda user_query: client.answer_question(\n",
    "        question=f\"\"\"{user_query} Think of a simple title for this presentation.\n",
    "        \n",
    "        Also, think of a good background colour, in RGB format,\\\n",
    "        for the slides and a good colour, also in RGB format, for the\\\n",
    "        text. Typically, if the text colour is bright (for example RGB [255, 255, 255] is white), then the background colour should be dark\n",
    "        (RGB [0, 0, 100] is dark blue). Conversely, if the text colour is dark (for example RGB [0, 0, 0] is black), the background colour should be bright\\\n",
    "        . You are free to choose any text and background colour, \\\n",
    "        as long as you follow these rules. Please do not assign grey-scale colours for the text and background (like RGB [50, 50, 50]), as much as possible.\n",
    "        \n",
    "        Format your reply as a JSON array containing the title and the two colours, following the example below. Do not say anything else.\n",
    "        Example:\n",
    "        [\"Slide Title\", {{\"background\": [100, 0, 0]}}, {{\"text\": [255, 255, 255]}}]\"\"\",\n",
    "    \n",
    "        system_prompt=f\"\"\"You are an assistant whose task is to help a user in creating a presentation.\\\n",
    "        Here are a list of wikipedia entry summaries that are selected for the presentation:\n",
    "        {chosen_snippets}\n",
    "        This should give you an idea of what this presentation should be about.\n",
    "        \"\"\",\n",
    "        llm='mistralai/Mixtral-8x7B-Instruct-v0.1' \n",
    ")\n",
    "\n",
    "format = try_and_parse(user_query, decide_slide_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40b715-2ef3-47f0-bcab-db6733e54873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e78e2a-08ef-4fed-8833-a6ad0296110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = RGBColor(*tuple(list(format[1].values())[0])) \n",
    "font = RGBColor(*tuple(list(format[2].values())[0])) \n",
    "fill = title_slide.background.fill\n",
    "fill.solid()\n",
    "fill.fore_color.rgb = background\n",
    "\n",
    "\n",
    "title_slide.shapes.title.text = format[0]\n",
    "title_slide.shapes.title.text_frame.paragraphs[0].font.color.rgb =  font\n",
    "title_slide.shapes.title.text_frame.paragraphs[0].font.name = 'Montserrat'\n",
    "title_slide.shapes.title.text_frame.paragraphs[0].font.bold = True\n",
    "\n",
    "first_shape =  title_slide.shapes[0]\n",
    "first_shape.left, first_shape.top, first_shape.width, first_shape.height = (prs.slide_width - Inches(12))//2, \\\n",
    "(prs.slide_height-first_shape.height)//2 - Inches(1),\\\n",
    "Inches(12),\\\n",
    "Inches(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebca46-abd0-45a9-9f13-82c9242dc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with client.connect(chat_session_id) as session:\n",
    "\n",
    "    for section in tqdm(sections):\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "        fill = slide.background.fill\n",
    "        fill.solid()\n",
    "        fill.fore_color.rgb = background\n",
    "\n",
    "        \n",
    "        contents = slide.placeholders[1]\n",
    "        contents.text_frame.word_wrap = True\n",
    "\n",
    "        title = slide.shapes.title\n",
    "        title.text = section\n",
    "        title.text_frame.paragraphs[0].font.color.rgb = font\n",
    "        title.text_frame.paragraphs[0].font.size = Pt(32)\n",
    "        title.text_frame.paragraphs[0].font.name = 'Karla'\n",
    "       \n",
    "       \n",
    "        content = session.query(\n",
    "            \n",
    "            message = section,\n",
    "            system_prompt=f\"\"\"You are an assistant whose task is to help a user in creating a presentation. \\\n",
    "            The slides of the presentation are as follows: {sections}\n",
    "            You are now tasked with generating the content of one slide, which will be provided by the user.\n",
    "            \"\"\",\n",
    "            pre_prompt_query=\"You have been provided with the following information, which may be useful in your task.\",\n",
    "            prompt_query=\"Decide if the information is relevant, and use it if needed. \\\n",
    "            Generate the content required in the slide provided by the user. You only need to generate the contents of the slide, not the title\\\n",
    "            or anything else. Generate a maximum of 3 paragraphs of text. Keep to a word limit of 250 words. \\\n",
    "            Do not use numbered lists.\",\n",
    "            llm=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            rag_config={\n",
    "                \"rag_type\": \"rag\",\n",
    "            },\n",
    "        ).content\n",
    "\n",
    "        contents.text = content\n",
    "        \n",
    "        for paragraph in contents.text_frame.paragraphs:\n",
    "            paragraph.space_after = 1\n",
    "            paragraph.space_before = 0\n",
    "           \n",
    "            paragraph.font.size = Pt(18)  \n",
    "            paragraph.font.color.rgb = font\n",
    "            paragraph.font.name = 'Karla'\n",
    "\n",
    "        contents.text_frame.auto_size = MSO_AUTO_SIZE.SHAPE_TO_FIT_TEXT\n",
    "        shapes = slide.shapes\n",
    "        new_width = Inches(14)\n",
    "        new_height = Inches(7)\n",
    "        shapes[0].height, shapes[0].width, shapes[0].top, shapes[0].left = shapes[0].height, new_width, shapes[0].top, (prs.slide_width-new_width)//2\n",
    "        shapes[1].height, shapes[1].width, shapes[1].top, shapes[1].left = new_height, new_width, shapes[1].top, (prs.slide_width-new_width)//2\n",
    "        \n",
    "        \n",
    "\n",
    "# gpt-4-1106-preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de66b9-768e-48c5-9e7d-2b7c5411d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitised = re.sub(r'[\\W_]+', '_', format[0])\n",
    "prs.save(f\"./presentations/{sanitised}.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65178226-d7a2-4259-ac1f-05f9c1a9cf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1291f22-071f-49dd-847e-264d34aa1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contents.text_frame.fit_text(max_size=17)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b21c2-909a-4fe1-9762-c18840ff2c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
